{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "<html><body>\n",
       "<h1>스크래핑</h1>\n",
       "<p>웹 페이지 분석</p>\n",
       "<p>원하는 부분 추출</p>\n",
       "</body></html>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = '''\n",
    "<html><body>\n",
    "<h1>스크래핑</h1>\n",
    "<p>웹 페이지 분석</p>\n",
    "<p>원하는 부분 추출</p>\n",
    "</body></html>\n",
    "'''\n",
    "\n",
    "soup=BeautifulSoup(html,'html.parser') # BeautifulSoup 객체 생성\n",
    "                                       # BeautifulSoup(분석 문서, 분석기 종류)\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<body>\n",
       "<h1>스크래핑</h1>\n",
       "<p>웹 페이지 분석</p>\n",
       "<p>원하는 부분 추출</p>\n",
       "</body>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 원하는 태그 명으로 묶인 부분 추출\n",
    "soup.body # .태그명"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<p>웹 페이지 분석</p>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.body.p #가장 처음으로 만나는 p태그만 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "<p>원하는 부분 추출</p>\n"
     ]
    }
   ],
   "source": [
    "# 형제 태그(동등한 위치에 있는 동일한 태그) 출력 방법\n",
    "p1=soup.body.p\n",
    "print(p1.next_sibling) # 한 번 쓰면 처음 태그 바로 뒤의 \\n 출력\n",
    "print(p1.next_sibling.next_sibling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'웹 페이지 분석'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1.string # 태그 없이 원하는 태그 내 문자열 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "스크래핑\n",
      "웹 페이지 분석\n"
     ]
    }
   ],
   "source": [
    "# find 함수 : 객체 명이 너무 길어지는 경우, id 이용해 직접 접근\n",
    "#             id 없어도 가능\n",
    "html = '''\n",
    "<html><body>\n",
    "<h1 id='title'>스크래핑</h1>\n",
    "<p id='body'>웹 페이지 분석</p>\n",
    "<p>원하는 부분 추출</p>\n",
    "</body></html>\n",
    "'''\n",
    "\n",
    "soup=BeautifulSoup(html,'html.parser')\n",
    "print(soup.find(id='title').string)\n",
    "print(soup.find(id='body').string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_all() : 여러개의 태그를 한 번에 추출\n",
    "html='''\n",
    "<html><body>\n",
    "<ul>\n",
    "<li><a href=\"http://www.naver.com\">naver</a></li>\n",
    "<li><a href=\"http://www.daum.net\">daum</a></li> \n",
    "</ul>\n",
    "</body></html>\n",
    "'''\n",
    "# ul/ol 태그 : 순서가 없는/있는 리스트\n",
    "# a 태그 : 태그로 묶인 내용을 클릭하면 해당 태그의 웹페이지로 연결\n",
    "# href : 속성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naver -> http://www.naver.com\n",
      "daum -> http://www.daum.net\n"
     ]
    }
   ],
   "source": [
    "soup=BeautifulSoup(html,'html.parser')\n",
    "links=soup.find_all('a')\n",
    "\n",
    "for a in links:\n",
    "    # if 'href' in a.attrs: : 원하는 속성 값이 있는지 확인코자 할 때\n",
    "    href=a.attrs['href'] # .attrs : 원하는 속성값 추출\n",
    "    text=a.string\n",
    "    print(text,'->',href)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "<html><body>\n",
       "<ul>\n",
       "<li><a href=\"http://www.naver.com\">naver</a></li>\n",
       "<li><a href=\"http://www.daum.net\">daum</a></li>\n",
       "</ul>\n",
       "</body></html>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기상청 육상 중기예보\n",
      "기압골의 영향으로 6일부터 8일 사이에 전국에 비 또는 눈이 오겠고, 제주도는 10~11일에도 비가 오겠습니다. <br />한편, 동풍의 영향으로 9일은 강원영동에 비 또는 눈이 오겠습니다. 그 밖의 날은 고기압의 가장자리에 들어 가끔 구름많겠습니다.<br />기온은 평년(최저기온: -12~0℃, 최고기온: 0~8℃)보다 높겠습니다.<br />강수량은 평년(0~3mm)보다 많겠습니다.\n"
     ]
    }
   ],
   "source": [
    "# 기상예보 데이터에서 특정 내용 추출\n",
    "import urllib.request as req\n",
    "\n",
    "url='http://www.weather.go.kr/weather/forecast/mid-term-rss3.jsp'\n",
    "res=req.urlopen(url)\n",
    "soup=BeautifulSoup(res, 'html.parser') # url 내용을 읽어오기 위해서는 BS 객체로 \n",
    "                                       # 만들어줘야 함\n",
    "title=soup.find('title').string\n",
    "wf=soup.find('wf').string\n",
    "print(title)\n",
    "print(wf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2020년'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# css 선택자 사용하기\n",
    "# soup.select_one(선택자) : 선택자로 지정된 요소 하나를 추출\n",
    "# soup.select_one(선택자) : 선택자로 지정된 요소 여러 개를추출\n",
    "html='''\n",
    "<html><body>\n",
    "<div id=\"myid\">\n",
    "<ul class='day'>\n",
    "<h1>2020년</h1>\n",
    "<li>월요일</li>\n",
    "<li>화요일</li>\n",
    "<li>수요일</li>\n",
    "</ul>\n",
    "</div>\n",
    "</body></html>\n",
    "'''\n",
    "\n",
    "soup=BeautifulSoup(html,'html.parser')\n",
    "soup.select_one('div#myid h1').string # find와 문법이 다르다는 점에 주의\n",
    "                                      # 동일한 태그가 없으면 h1만 써도 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ul class=\"day\">\n",
       "<h1>2020년</h1>\n",
       "<li>월요일</li>\n",
       "<li>화요일</li>\n",
       "<li>수요일</li>\n",
       "</ul>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.select_one('div#myid ul.day') # 아이디는 #뒤에, 클래스는 .뒤에 써줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "서시\n",
      "자화상\n",
      "소년\n",
      "눈 오는 지도\n",
      "돌아와 보는 밤\n",
      "병원\n",
      "새로운 길\n",
      "간판 없는 거리\n",
      "태초의 아침\n",
      "또 태초의 아침\n",
      "새벽이 올 때까지\n",
      "무서운 시간\n",
      "십자가\n",
      "바람이 불어\n",
      "슬픈 족속\n",
      "눈감고 간다\n",
      "또 다른 고향\n",
      "길\n",
      "별 헤는 밤\n"
     ]
    }
   ],
   "source": [
    "url='https://ko.wikisource.org/wiki/%EC%A0%80%EC%9E%90:%EC%9C%A4%EB%8F%99%EC%A3%BC'\n",
    "res=req.urlopen(url)\n",
    "soup=BeautifulSoup(res,'html.parser')\n",
    "\n",
    "addr='#mw-content-text > div > ul > li ul li' # '>' : 한 ul 내 li / ' ' : 모든 ul 내 li\n",
    "arti=soup.select(addr)\n",
    "for a in arti:\n",
    "    print(a.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Python'"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 다양한 추출 방법\n",
    "html='''\n",
    "<ul id='language'>\n",
    "    <li id='bas'>Basic</li>\n",
    "    <li id='cpp'>C++</li>\n",
    "    <li id='ja'>Java</li>\n",
    "    <li id='py'>Python</li>\n",
    "    <li id='sp'>Spark</li>\n",
    "</ul>\n",
    "'''\n",
    "sel=BeautifulSoup(html,'html.parser')\n",
    "\n",
    "sel.select_one('#py').string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python\n",
      "Python\n",
      "Python\n",
      "Python\n",
      "Python\n",
      "Python\n",
      "Python\n"
     ]
    }
   ],
   "source": [
    "myFunc=lambda arg:print(sel.select_one(arg).string)\n",
    "myFunc('#py')\n",
    "myFunc('li#py')\n",
    "myFunc('ul li#py')\n",
    "myFunc('#language #py')\n",
    "myFunc('ul#language li#py')\n",
    "myFunc('li[id=py]')\n",
    "myFunc('li:nth-of-type(4)') # 브라우저별로 안되는 경우가 있어서 비추"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Python'"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel.select('li')[3].string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Python'"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel.find_all('li')[3].string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아보카도\n",
      "아보카도\n",
      "아보카도\n",
      "아보카도\n",
      "아보카도\n"
     ]
    }
   ],
   "source": [
    "fp=open('fru-veg.html',encoding='utf-8')\n",
    "soup=BeautifulSoup(fp,'html.parser')\n",
    "\n",
    "# select 활용\n",
    "print(soup.select_one('#ve li:nth-of-type(4)').string)\n",
    "print(soup.select('ul li[data-lo=us]')[3].string)\n",
    "print(soup.select('ul li.black')[0].string)\n",
    "\n",
    "# find 활용\n",
    "cond={'data-lo':'us','class':'black'} # 조건을 딕셔너리로 만들어 적용 가능\n",
    "print(soup.find('li',cond).string)\n",
    "print(soup.find(id='ve').find('li',cond).string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"https://test.html\">test2</a>, <a href=\"https://test.html\">test3</a>]"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정규식 표현과 함께 사용\n",
    "import re\n",
    "\n",
    "html='''\n",
    "<li><a href='test.html'>test</li>\n",
    "<li><a href='https://test.html'>test2</li>\n",
    "<li><a href='https://test.html'>test3</li>\n",
    "<li><a href='http://test.html'>test4</li>\n",
    "'''\n",
    "\n",
    "soup=BeautifulSoup(html,'html.parser')\n",
    "soup.find_all(href=re.compile(\"https://\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 다음의 조건을 만족하는 Point라는 클래스를 작성하세요.\n",
    "# Point 클래스는 생성자(__init__)를 통해 (x, y) 좌표를 입력받는다.\n",
    "# setx(x), sety(y) 메서드를 통해 x 좌표와 y 좌표를 따로 입력받을 수도 있다.\n",
    "# get() 메서드를 호출하면 튜플로 구성된 (x, y) 좌표를 반환한다.\n",
    "# move(dx, dy) 메서드는 현재 좌표를 dx, dy만큼 이동시킨다.\n",
    "\n",
    "class Point:\n",
    "    def __init__(self):\n",
    "        self.x=0\n",
    "        self.y=0\n",
    "    def setx(self, a):\n",
    "        self.x=a\n",
    "    def sety(self, b):\n",
    "        self.y=b\n",
    "    def move(self, dx, dy):\n",
    "        self.x+=dx\n",
    "        self.y+=dy\n",
    "    def get(self):\n",
    "        return self.x, self.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, -2)"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Point 클래스에 대한 인스턴스를 생성한 후 4개의 메서드를 사용하는 코드를 작성\n",
    "\n",
    "posi=Point()\n",
    "posi.setx(3)\n",
    "posi.sety(-5)\n",
    "posi.move(2,3)\n",
    "posi.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 1부터 10까지의 숫자를 각 라인 단위로 파일에 출력하는 프로그램을 작성\n",
    "# 생성되는 파일의 이름은 number.txt\n",
    "\n",
    "with open('number.txt','w') as f1:\n",
    "    for i in range(1,11):\n",
    "        f1.write(str(i)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users\\student\\Desktop\\TIL_new\\Python 기초\n"
     ]
    }
   ],
   "source": [
    "# 4. 경로를 입력받은 후 해당 경로에 있는 디렉터리와 파일 목록을 flist.txt라는\n",
    "# 파일로 출력하는 함수를 작성\n",
    "\n",
    "import glob\n",
    "\n",
    "path=input()\n",
    "with open('flist.txt','w') as f2:\n",
    "    files=glob.glob(path+'\\*')\n",
    "    for i in files:\n",
    "        f2.write(i+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 윤동주 시인 방송 출연 년월일 추출(화요일 수업내용 중)\n",
    "\n",
    "# 미완성 코드 : 1박 2일에서 2일도 같이 출력되어버림\n",
    "# from urllib import request as req\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# url='https://ko.wikipedia.org/wiki/%EC%9C%A4%EB%8F%99%EC%A3%BC#%EB%B0%A9%EC%86%A1'\n",
    "# temp=req.urlopen(url)\n",
    "# soup=BeautifulSoup(temp,'html.parser')\n",
    "# selector='#mw-content-text > div > ul:nth-child(71) li'\n",
    "# whole=soup.select(selector)\n",
    "\n",
    "# res=[]\n",
    "# for i in whole:\n",
    "#     j=str(i)\n",
    "#     pat1=re.search('\\d+년',j).group()\n",
    "#     pat2=re.search('\\d+월',j).group()\n",
    "#     pat3=re.search('\\d+일',j).group()\n",
    "#     res.append(str(pat1)+' '+str(pat2)+' '+str(pat3))\n",
    "# res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1984년 12월 22일',\n",
       " '1988년 3월 1일',\n",
       " '1995년 3월 11일',\n",
       " '2006년 7월 31일',\n",
       " '2009년 8월 15일',\n",
       " '2011년 11월 4일',\n",
       " '2016년 3월 6일']"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. 윤동주 시인 방송 출연 년월일 추출(화요일 수업내용 중)\n",
    "\n",
    "from urllib import request as req\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url='https://ko.wikipedia.org/wiki/%EC%9C%A4%EB%8F%99%EC%A3%BC#%EB%B0%A9%EC%86%A1'\n",
    "temp=req.urlopen(url)\n",
    "soup=BeautifulSoup(temp,'html.parser')\n",
    "selector='#mw-content-text > div > ul:nth-child(71) > li'\n",
    "whole=soup.select(selector)\n",
    "\n",
    "text=[]\n",
    "for i in whole:\n",
    "    text.append(i.get_text()) # 텍스트 정보만 text에 출력\n",
    "\n",
    "res=[]\n",
    "for j in text:\n",
    "    pat=re.search('\\d+년 \\d+월 \\d+일',j)\n",
    "    if pat!=None:\n",
    "        res.append(str(pat.group()))\n",
    "res    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기압골의 영향으로 6일부터 8일 사이에 전국에 비 또는 눈이 오겠고, 제주도는 10~11일에도 비가 오겠습니다. <br />한편, 동풍의 영향으로 9일은 강원영동에 비 또는 눈이 오겠습니다. 그 밖의 날은 고기압의 가장자리에 들어 가끔 구름많겠습니다.<br />기온은 평년(최저기온: -12~0℃, 최고기온: 0~8℃)보다 높겠습니다.<br />강수량은 평년(0~3mm)보다 많겠습니다.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'기압골의 영향으로 일부터 일 사이에 전국에 비 또는 눈이 오겠고 제주도는 일에도 비가 오겠습니다  한편 동풍의 영향으로 일은 강원영동에 비 또는 눈이 오겠습니다 그 밖의 날은 고기압의 가장자리에 들어 가끔 구름많겠습니다 기온은 평년최저기온  최고기온 보다 높겠습니다 강수량은 평년보다 많겠습니다'"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6. 영문, 숫자 포함하여 특수문자 모두 제거(오늘 수업내용 중)\n",
    "\n",
    "url='http://www.weather.go.kr/weather/forecast/mid-term-rss3.jsp'\n",
    "res=req.urlopen(url)\n",
    "soup=BeautifulSoup(res, 'html.parser')\n",
    "text=soup.select(selector)\n",
    "\n",
    "#wf태그값 추출\n",
    "wf_tag=soup.find('wf').string\n",
    "print(wf_tag)\n",
    "\n",
    "pat=re.compile('[가-힣\\s]+')\n",
    "res=pat.findall(str(wf_tag))\n",
    "only_ko=''\n",
    "for i in res:\n",
    "    only_ko+=i\n",
    "only_ko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
